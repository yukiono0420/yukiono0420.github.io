<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Categories on ラビットチャレンジレポート提出用サイト</title>
    <link>https://yukiono0420.github.io/categories/</link>
    <description>Recent content in Categories on ラビットチャレンジレポート提出用サイト</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    
	<atom:link href="https://yukiono0420.github.io/categories/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    <item>
      <title>機械学習レポート</title>
      <link>https://yukiono0420.github.io/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</link>
      <pubDate>Sat, 12 Jun 2021 01:18:19 +0900</pubDate>
      
      <guid>https://yukiono0420.github.io/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</guid>
      <description>線形回帰モデル（単回帰モデル） 講義内容サマリ 回帰問題 回帰モデルではある入力値（離散あるいは連続）から出力値（連続）を予測する問題であり、線形関数（直線）で予測するものを線形回帰（単回帰）という。
 回帰で扱うデータ  入力値（各要素を説明変数または特徴量と呼ぶ）  m次元ベクトル（m=1の場合はスカラー）   出力値（目的変数）  スカラー値（目的変数）      入力値（説明変数） $$ \begin{eqnarray*} \boldsymbol{x} &amp;amp;=&amp;amp; (x_1, x_2, \cdots, x_m)^{T} \\ \end{eqnarray*} $$
出力値（目的変数） $$ \begin{eqnarray*} y \in \mathbb{R}^m \\ \end{eqnarray*} $$
線形回帰モデル 線形回帰モデルとは、機械学習モデルの一つであり、教師あり学習の回帰手法である。 入力値に対して$m$次元パラメータの線型結合を出力するモデルである。 パラメータとは、モデルに含まれる推定すべき未知のパラメータであり、特徴量が予測値に対してどのように影響を与えるかを決定する重み値の集合である。 線形回帰モデルでは、この未知のパラメータに対して、最小二乗法により推定する。
パラメータ $$ \begin{eqnarray*} \boldsymbol{w} &amp;amp;=&amp;amp; (w_1, w_2, \cdots, w_m)^{T} \in \mathbb{R}^m \\ \end{eqnarray*} $$
線型結合 $$ \begin{eqnarray*} \hat{y} = \boldsymbol{w}^T \boldsymbol{x} + w_0 = \sum^{m}_{j=1} w_j x_j + w_0 \\ \end{eqnarray*} $$</description>
    </item>
    
    
    
    <item>
      <title>応用数学レポート</title>
      <link>https://yukiono0420.github.io/%E5%BF%9C%E7%94%A8%E6%95%B0%E5%AD%A6%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</link>
      <pubDate>Mon, 07 Jun 2021 00:56:22 +0900</pubDate>
      
      <guid>https://yukiono0420.github.io/%E5%BF%9C%E7%94%A8%E6%95%B0%E5%AD%A6%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</guid>
      <description>第1章　線形代数 スカラーとベクトルの違いや行列の基本計算方法（四則演算）の習得後、行列の固有値・固有ベクトルの算出方法について習得。 また、正方行列以外の行列に対する固有値・固有ベクトルの算出方法として、特異値・特異ベクトルの算出方法について習得。 最後に、アインシュタインの画像を用いて、特異値分解が実際に利用されている場面について紹介があり、特異値分解が実際の画像処理の場面でどのように活用されているかを習得する事ができた。
第2章　確率・統計 基本的な集合の概念や確率の基本的な求め方について習得した後、条件付き確率について習得した。 「洗濯物を干している日に雨が降る確率」と「洗濯物を干しているかつ雨が降る確率」が異なるという演習の問題を通じて、条件付き確率と独立な事象の同時確率の違いについて理解を深めた。 ベイズの定理（講義内ではベイズ則）については、結果から原因を推定する際に利用されていると認識している。
また、標本分散と不偏分散の違いの部分での説明は、講師が口頭にてわかりやすく説明。 不偏分散と標本分散では、分母がn-1とnで異なるが、これは分散が標本平均と各標本との差分の二乗を合計しているからで、標本平均を導出する際にすでに自由度が1減少していることから、不偏分散では分母が標本数のnではなくn-1が採用されたと理解した。最後に、様々な分布の説明では、ベルヌーイ分布・二項分布・正規分布などの具体的な数式について習得した。
第3章　情報理論 情報理論についてはこれまであまり触れた事がない分野だったため、新鮮だった。 自己情報量は、元の情報量からどれくらい増加しているかを比率で考え、それを数式化する際には log をとるということを習得。 （logの底が2の場合は単位がbit、logの底がeの場合は単位がnat） また、シャノンエントロピーは自己情報量の期待値という定義について習得し、シャノンエントロピーの定性的な意味について理解した。 カルバック・ライブラー・ダイバージェンスは、同じ事象・確率変数における異なる確率分布の違いについて、定量的に表したものと理解。 カルバック・ライブラー・ダイバージェンスの中で、古い確率分布で見た自己情報量を新しい確率分布で平均した項は、交差エントロピーということを理解した。</description>
    </item>
    
    
    
    
  </channel>
</rss>
