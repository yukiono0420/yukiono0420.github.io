<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Categories on ラビットチャレンジレポート提出用サイト</title>
    <link>https://yukiono0420.github.io/categories/</link>
    <description>Recent content in Categories on ラビットチャレンジレポート提出用サイト</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    
	<atom:link href="https://yukiono0420.github.io/categories/index.xml" rel="self" type="application/rss+xml" />
    
    
    
    <item>
      <title>深層学習レポートDay3</title>
      <link>https://yukiono0420.github.io/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88day3/</link>
      <pubDate>Sun, 04 Jul 2021 23:36:41 +0900</pubDate>
      
      <guid>https://yukiono0420.github.io/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88day3/</guid>
      <description>0. 深層学習全体像の復習 最新のCNN: AlexNet AlexNetとは2012年威開かれた画像認識コンペティションで2位に大差をつけて優勝したモデルである。AlexNetの登場で、ディープラーニングが大きな注目を集めたきっかけとなった。AlexNetのモデル構造は、5層の畳み込み層およびプーリング層など、それに続く3層の全結合層から構成される。AlexNetでは、過学習を防ぐ施策として、サイズ4096の全結合層の出力にドロップアウトを使用している。
確認テスト サイズ5×5の入力画像を、サイズ3×3のフィルタで畳み込んだ時の出力画像のサイズを答えよ。なおストライドは2、パディングは1とする。
回答 3×3
Section1. 再帰型ニューラルネットワークについて 1.1 RNN全体像 1.1.1 RNNとは RNN（再帰型ニューラルネットワーク）とは、時系列データに対応可能な、ニューラルネットワークである。
1.1.2 時系列データ 時系列データとは、時間的順序をおって一定間隔ごとに観測され、さらに相互に統計的依存関係が認められるようなデータ系列のことを指す。
 具体例  音声データ テキストデータ    1.1.3 RNNの全体像 RNNでは、時刻t+1における中間層の重み$z_{t+1}$更新の際に、時刻tにおける中間層の重み$z_t$を入力データとして使用している。RNNの特徴としては、時系列モデルを扱うには、初期の状態と過去の時間t-1の状態を保持し、そこから次の時間でのtを再帰的に求める再帰構造が必要になる。
 RNN構造    RNNを数式で表したもの   確認テスト1 RNNのネットワークには大きくわけて3つの重みがある。1つは入力から現在の中間層を定義する際にかけられる重み、1つは中間層から出力を定義する際にかけられる重みである。残り1つの重みについて説明せよ。
回答 1つ前の時点の中間層から、現在の中間層を定義する際にかけられる重み
1.2 BPTT(Backpropagation Through Time) 1.2.1 BPTTとは BPTT(Backpropagation Through Time)とは、RNNにおいてのパラメータの調整方法の一つであり、時間軸方向への誤差逆伝播法である。
確認テスト2 連鎖律の原理を使い、dz/dxを求めよ。
回答 dz/dx = dz/dt * dt/dx = 2t * 1 = 2(x + y)
1.2.2 BPTTの数学的記述 パラメータの更新式は以下のようになる。  BPTTにおける各パラメータの更新式   確認テスト3 下図のy1をx・s0・s1・win・w・woutを用いて数式で表せ。※バイアスは任意の文字で定義せよ。※また中間層の出力にシグモイド関数g(x)を作用させよ。</description>
    </item>
    
    
    
    <item>
      <title>深層学習レポートDay1Day2</title>
      <link>https://yukiono0420.github.io/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88day1day2/</link>
      <pubDate>Sun, 27 Jun 2021 16:15:35 +0900</pubDate>
      
      <guid>https://yukiono0420.github.io/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88day1day2/</guid>
      <description>1. 深層学習day1レポート 1.0 ニューラルネットワークの全体像 確認テスト1 ディープラーニングは、結局何をやろうとしているか2行以内で述べよ。また、次の中のどの値の最適化が最終目的か。全て選べ。（1分） ①入力値[ X] ②出力値[ Y]③重み[W]④バイアス[b]⑤総入力[u] ⑥中間層入力[ z]⑦学習率[ρ]
回答 多数の中間層を用いることにより、入力から目的とする数値を出力する変換を行う数学モデルを構築すること。③と④。
確認テスト2 次のネットワークを紙にかけ。
入力層 2ノード1層
中間層 ３ノード2層
出力層 1ノード1層（5分）
NN全体像確認テスト
回答  ネットワーク   1.1 入力層〜中間層 ある入力の特徴の入力を各ノードに代入し、その入力に対して重みwとバイアスbを考慮し、中間層のノードに対して出力を行う。動物の写真判定の例では、入力として「動物の体長」や「動物の足の長さ」など、写真を判定するのに必要な情報をインプットする。
確認テスト3 この図式に動物分類の実例を入れてみよう。（3分）  確認テスト問題   回答  確認テスト回答   確認テスト4 この数式をPythonで書け。（2分）
回答  確認テスト問題   u = np.dot(x,W) + b 確認テスト5 1-1のファイルから中間層の出力を定義しているソースを抜き出せ。（2分）
回答 # 中間層出力 z = functions_dnn.relu(u) print_vec(&amp;#34;中間層出力&amp;#34;, z) 実装演習 単層単ユニットにおける実装演習結果を示す。
#functionsモジュールを呼び出すことができなかったため、functionsをfunctions_dnnに読み替えて実装 import numpy as np #from common import functions_dnn import functions_dnn def print_vec(text, vec): print(&amp;#34;*** &amp;#34; + text + &amp;#34; ***&amp;#34;) print(vec) #print(&amp;#34;shape: &amp;#34; + str(x.</description>
    </item>
    
    
    
    <item>
      <title>機械学習レポート</title>
      <link>https://yukiono0420.github.io/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</link>
      <pubDate>Sat, 12 Jun 2021 01:18:19 +0900</pubDate>
      
      <guid>https://yukiono0420.github.io/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</guid>
      <description>1.線形回帰モデル
2.非線形回帰モデル
3.ロジスティック回帰モデル
4.主成分分析
5.アルゴリズム
6.サポートベクタマシン
線形回帰モデル &amp;lt;講義内容サマリ&amp;gt;
回帰問題 回帰モデルではある入力値（離散あるいは連続）から出力値（連続）を予測する問題であり、線形関数（直線）で予測するものを線形回帰（単回帰）という。
 回帰で扱うデータ  入力値（各要素を説明変数または特徴量と呼ぶ）  m次元ベクトル（m=1の場合はスカラー）   出力値（目的変数）  スカラー値（目的変数）      入力値（説明変数） $$ \begin{eqnarray*} \boldsymbol{x} &amp;amp;=&amp;amp; (x_1, x_2, \cdots, x_m)^{T} \\ \end{eqnarray*} $$
出力値（目的変数） $$ \begin{eqnarray*} y \in \mathbb{R}^m \\ \end{eqnarray*} $$
線形回帰モデルとは 線形回帰モデルとは、機械学習モデルの一つであり、教師あり学習の回帰手法である。 入力値に対して$m$次元パラメータの線型結合を出力するモデルである。 パラメータとは、モデルに含まれる推定すべき未知のパラメータであり、特徴量が予測値に対してどのように影響を与えるかを決定する重み値の集合である。 線形回帰モデルでは、この未知のパラメータに対して、最小二乗法により推定する。
パラメータ $$ \begin{eqnarray*} \boldsymbol{w} &amp;amp;=&amp;amp; (w_1, w_2, \cdots, w_m)^{T} \in \mathbb{R}^m \\ \end{eqnarray*} $$
線型結合 $$ \begin{eqnarray*} \hat{y} = \boldsymbol{w}^T \boldsymbol{x} + w_0 = \sum^{m}_{j=1} w_j x_j + w_0 \\ \end{eqnarray*} $$</description>
    </item>
    
    
    
    <item>
      <title>応用数学レポート</title>
      <link>https://yukiono0420.github.io/%E5%BF%9C%E7%94%A8%E6%95%B0%E5%AD%A6%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</link>
      <pubDate>Mon, 07 Jun 2021 00:56:22 +0900</pubDate>
      
      <guid>https://yukiono0420.github.io/%E5%BF%9C%E7%94%A8%E6%95%B0%E5%AD%A6%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88/</guid>
      <description>第1章　線形代数 スカラーとベクトルの違いや行列の基本計算方法（四則演算）の習得後、行列の固有値・固有ベクトルの算出方法について習得。 また、正方行列以外の行列に対する固有値・固有ベクトルの算出方法として、特異値・特異ベクトルの算出方法について習得。 最後に、アインシュタインの画像を用いて、特異値分解が実際に利用されている場面について紹介があり、特異値分解が実際の画像処理の場面でどのように活用されているかを習得する事ができた。
第2章　確率・統計 基本的な集合の概念や確率の基本的な求め方について習得した後、条件付き確率について習得した。 「洗濯物を干している日に雨が降る確率」と「洗濯物を干しているかつ雨が降る確率」が異なるという演習の問題を通じて、条件付き確率と独立な事象の同時確率の違いについて理解を深めた。 ベイズの定理（講義内ではベイズ則）については、結果から原因を推定する際に利用されていると認識している。
また、標本分散と不偏分散の違いの部分での説明は、講師が口頭にてわかりやすく説明。 不偏分散と標本分散では、分母がn-1とnで異なるが、これは分散が標本平均と各標本との差分の二乗を合計しているからで、標本平均を導出する際にすでに自由度が1減少していることから、不偏分散では分母が標本数のnではなくn-1が採用されたと理解した。最後に、様々な分布の説明では、ベルヌーイ分布・二項分布・正規分布などの具体的な数式について習得した。
第3章　情報理論 情報理論についてはこれまであまり触れた事がない分野だったため、新鮮だった。 自己情報量は、元の情報量からどれくらい増加しているかを比率で考え、それを数式化する際には log をとるということを習得。 （logの底が2の場合は単位がbit、logの底がeの場合は単位がnat） また、シャノンエントロピーは自己情報量の期待値という定義について習得し、シャノンエントロピーの定性的な意味について理解した。 カルバック・ライブラー・ダイバージェンスは、同じ事象・確率変数における異なる確率分布の違いについて、定量的に表したものと理解。 カルバック・ライブラー・ダイバージェンスの中で、古い確率分布で見た自己情報量を新しい確率分布で平均した項は、交差エントロピーということを理解した。</description>
    </item>
    
    
    
    
  </channel>
</rss>
