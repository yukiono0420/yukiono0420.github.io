<!DOCTYPE html>
<html><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="E検定チャレンジ用">
    
    <link rel="shortcut icon" href="https://yukiono0420.github.io/favicon.ico">
    
    <link rel="stylesheet" href="/css/style.min.css">

    <title>機械学習レポート</title>
</head>
<body><head>
    
	
		<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$']],
            processEscapes: true,
            processEnvironments: true,
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            TeX: { equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"] }
        }
    });

    MathJax.Hub.Queue(function() {
        
        
        
        var all = MathJax.Hub.getAllJax(), i;
        for(i = 0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });

    MathJax.Hub.Config({
        
        TeX: { equationNumbers: { autoNumber: "AMS" } }
    });

</script>

<link rel="stylesheet" type="text/css" href="https://yukiono0420.github.iocss/mathjax-style.css">

	
</head>

<main id="content">
<article>
    <header id="post-header">
        <h1>機械学習レポート</h1>
            <div>
                <time>June 12, 2021</time>
                </div>
    </header><p><a href="#%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0%E3%83%A2%E3%83%87%E3%83%AB">1.線形回帰モデル</a></p>
<p><a href="#%E9%9D%9E%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0%E3%83%A2%E3%83%87%E3%83%AB">2.非線形回帰モデル</a></p>
<p><a href="#%E3%83%AD%E3%82%B8%E3%82%B9%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E5%9B%9E%E5%B8%B0%E3%83%A2%E3%83%87%E3%83%AB">3.ロジスティック回帰モデル</a></p>
<p><a href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90">4.主成分分析</a></p>
<p><a href="#%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0">5.アルゴリズム</a></p>
<p><a href="#%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%82%BF%E3%83%9E%E3%82%B7%E3%83%B3">6.サポートベクタマシン</a></p>
<h1 id="線形回帰モデル">線形回帰モデル</h1>
<p>&lt;講義内容サマリ&gt;</p>
<h3 id="回帰問題">回帰問題</h3>
<p>回帰モデルではある入力値（離散あるいは連続）から出力値（連続）を予測する問題であり、線形関数（直線）で予測するものを線形回帰（単回帰）という。</p>
<ul>
<li>回帰で扱うデータ
<ul>
<li>入力値（各要素を説明変数または特徴量と呼ぶ）
<ul>
<li>m次元ベクトル（m=1の場合はスカラー）</li>
</ul>
</li>
<li>出力値（目的変数）
<ul>
<li>スカラー値（目的変数）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>入力値（説明変数）</strong>
<code>$$ \begin{eqnarray*} \boldsymbol{x} &amp;=&amp; (x_1, x_2, \cdots, x_m)^{T} \\ \end{eqnarray*} $$</code></p>
<p><strong>出力値（目的変数）</strong>
<code>$$ \begin{eqnarray*} y \in \mathbb{R}^m \\ \end{eqnarray*} $$</code></p>
<h3 id="線形回帰モデルとは">線形回帰モデルとは</h3>
<p>線形回帰モデルとは、機械学習モデルの一つであり、教師あり学習の回帰手法である。
入力値に対して$m$次元パラメータの線型結合を出力するモデルである。
パラメータとは、モデルに含まれる推定すべき未知のパラメータであり、特徴量が予測値に対してどのように影響を与えるかを決定する重み値の集合である。
線形回帰モデルでは、この未知のパラメータに対して、最小二乗法により推定する。</p>
<p><strong>パラメータ</strong>
<code>$$ \begin{eqnarray*} \boldsymbol{w} &amp;=&amp; (w_1, w_2, \cdots, w_m)^{T} \in \mathbb{R}^m \\ \end{eqnarray*} $$</code></p>
<p><strong>線型結合</strong>
<code>$$ \begin{eqnarray*} \hat{y} = \boldsymbol{w}^T \boldsymbol{x} + w_0 = \sum^{m}_{j=1} w_j x_j + w_0 \\ \end{eqnarray*} $$</code></p>
<h3 id="データの分割とモデルの汎化性能測定">データの分割とモデルの汎化性能測定</h3>
<p>教師用データについて、学習用データと検証用データにそれぞれ分割する。モデルの汎化性能（Generalization）を測定するためで、データへの当てはまりの良さではなく、未知のインプットデータに対しての精度の高さを測定する。</p>
<ul>
<li>学習用データ：機械学習モデルの学習に利用するデータ</li>
<li>検証用データ：学習済みモデルの制度を検証するためのデータ</li>
</ul>
<h3 id="線形回帰モデルパラメータの推定">線形回帰モデルパラメータの推定</h3>
<p>モデルパラメータの推定は、以下の平均二乗誤差MSE$_{train}$（残渣平方和：データとモデル出力の二乗誤差の和）を最小化するパラメータを探索する。
<code>$$ \begin{eqnarray*} \rm{MSE}_{train} = \frac{1}{n_{train}} \sum^{n_{train}}_{i=1} (\hat{y}^{(train)}_i - y^{(train)}_i)^2 \\  \end{eqnarray*} $$</code></p>
<p>MSEを最小とするような$w$を求めるため、以下の方程式を解く。
\begin{eqnarray*}
\frac{\partial}{\partial w}\rm{MST_{train}} = 0 \<br>
\end{eqnarray*}
左辺について行列変形を実施する。
<code>\begin{eqnarray*} \frac{\partial}{\partial w}\rm{MST_{train}} &amp;=&amp;  \frac{\partial}{\partial w} \left(\frac{1}{n_{\rm{train}}} \sum^{n_{\rm{(train)}}}_{i=1} \left(\hat{y}^{\rm{(train)}}_i - y^{\rm{(train)}}_i\right)^2 \right) \\ &amp;=&amp; \frac{\partial}{\partial \boldsymbol{w}} \left( \left(X^{n_{\rm{(train)}}} \boldsymbol{w} - \boldsymbol{y}^{(\rm{train})} \right)^T \left(X^{n_{\rm{(train)}}} \boldsymbol{w} - \boldsymbol{y}^{(\rm{(train)})} \right) \right) \\ &amp;=&amp; 0  \\ \end{eqnarray*}</code>
これを解くと以下の回帰係数の式が得られる。</p>
<ul>
<li>
<p>回帰係数
\begin{eqnarray*}
\boldsymbol{\hat{w}} &amp;=&amp; \left({X^{\rm{(train)}}}^{T} X^{\rm{(train)}}\right)^{-1} {X^{\rm{(train)}}}^{T} \boldsymbol{y}^{\rm{(train)}} \<br>
\end{eqnarray*}</p>
</li>
<li>
<p>予測値
\begin{eqnarray*}
\boldsymbol{\hat{y}} &amp;=&amp; X\left({X^{\rm{(train)}}}^{T} X^{\rm{(train)}}\right)^{-1} {X^{\rm{(train)}}}^{T} \boldsymbol{y}^{\rm{(train)}} \<br>
\end{eqnarray*}</p>
</li>
</ul>
<p>&lt;実装演習結果&gt;
ボストンの家賃価格に関する統計情報を元に、線形回帰モデルの演習を実施する。
今回は、部屋数と家賃との関係性に着目して、線形回帰モデルにてフィッティングを実施する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span>  <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span> <span class="p">,</span><span class="n">columns</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="c1"># 目的変数をDataFrameへ追加</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;PRICE&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;RM&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;PRICE&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>

<span class="c1">#線形回帰モデルにてフィッティングを実行</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div><p>Room数が3部屋だった場合の家賃を予想する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">3</span><span class="p">]])</span>
</code></pre></div><p>array([[-7.36429383]])</p>
<p>部屋数と家賃価格との関係性をプロットする。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>
</code></pre></div><figure class="center"><img src="/image/lin_fit.png" width="550" height="350"/><figcaption>
            <h4>部屋数に対する家賃価格のプロットと線形回帰結果</h4>
        </figcaption>
</figure>

<h1 id="非線形回帰モデル">非線形回帰モデル</h1>
<p>&lt;講義内容サマリ&gt;</p>
<p>線形回帰で実現できないような複雑な非線形構造を内在する現象に対して、非線形回帰モデリングを実施する。
非線形回帰モデルでは、回帰関数として基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線型結合を使用する。未知パラメータは線形回帰モデルと応用に最小二乗法や最尤法によって推定する。
基底展開法も線形回帰と同じ枠組みで推定可能である。</p>
<p>\begin{eqnarray*}
y_i = w_0 + \sum^{m}_{j=1} w_j \phi_j (\boldsymbol{x}_i) + \epsilon_i \<br>
\end{eqnarray*}</p>
<ul>
<li>よく使われる基底関数
<ul>
<li>多項式関数</li>
<li>ガウス型基底関数</li>
<li>スプライン関数/Bスプライン関数</li>
</ul>
</li>
</ul>
<p>説明変数
\begin{eqnarray*}
x_i = (x_{i1},x_{i2},\cdots, x_{im}) \in \mathbb{R}^m \<br>
\end{eqnarray*}
非線形関数ベクトル
\begin{eqnarray*}
\phi (\boldsymbol{x_i}) = (\phi_1(\boldsymbol{x}_i), \phi_2 (\boldsymbol{x}_i), \cdots, \phi_k (\boldsymbol{x}_i))^T \in \mathbb{R}^k \<br>
\end{eqnarray*}
非線形関数の計画行列
\begin{eqnarray*}
\Phi^{(train)} = (\Phi (\boldsymbol{x}_1), \Phi (\boldsymbol{x}_2), \cdots, \Phi (\boldsymbol{x}_n))^T \in \mathbb{R}^{n \times k} \<br>
\end{eqnarray*}
最尤法による予測値
\begin{eqnarray*}
\hat{\boldsymbol{y}} = \Phi \left( \Phi^{(train)T} \Phi^{(train)} \right)^{-1} \Phi^{(train)T} \boldsymbol{y}^{(train)} \<br>
\end{eqnarray*}</p>
<h4 id="未学習underfitting-と過学習-overfitting">未学習（underfitting) と過学習 (overfitting)</h4>
<ul>
<li>学習データに対して、十分小さな誤差が得られないモデル$\rightarrow$未学習
<ul>
<li>（対策）モデルの表現力が低いため、表現力の高いモデルを利用する</li>
</ul>
</li>
<li>小さな誤差は得られたけど、テスト集合誤差との差が大きいモデル$\rightarrow$過学習
<ul>
<li>(対策1) 学習データの数を増やす</li>
<li>(対策2) 不要な基底関数（変数）を削除して表現力を抑止</li>
<li>(対策3) 正則化法を利用して表現力を抑止</li>
</ul>
</li>
</ul>
<h5 id="対策2不要な基底関数を削除する">（対策2）不要な基底関数を削除する</h5>
<ul>
<li>基底関数の数、位置やバンド幅によるモデルの複雑さが変化する</li>
<li>解きたい問題に対して多くの基底関数を用意してしまうと過学習の問題が起こるため、適切な基底関数を用意する</li>
</ul>
<h5 id="対策3正則化法罰則化法">（対策3）正則化法（罰則化法）</h5>
<ul>
<li>
<p>「モデルの複雑さに伴って、その値が大きくなる正則化項（罰則項）を課した関数」を最小化する
\begin{eqnarray*}
S_{\gamma} = \left(\boldsymbol{y} - \Phi \boldsymbol{w}\right)^T \left( \boldsymbol{y} - \Phi \boldsymbol{w} \right) + \gamma R(\boldsymbol{w})
\end{eqnarray*}
基底関数の数kが増加するとパラメータが増加し、残渣が減少する（モデルが複雑化）。</p>
</li>
<li>
<p>正則化</p>
<ul>
<li>L2ノルムを利用 $\rightarrow$ Ridge推定量</li>
<li>L1ノルムを利用 $\rightarrow$ Lasso推定量</li>
</ul>
</li>
</ul>
<h4 id="汎化性能">汎化性能</h4>
<p>適切なモデル（汎化性能が高いモデル）は交差検証法にて決定する。汎化性能とは、学習に使用した入力だけではなく、これまで見たことのない新たな入力に対する予測性能のことである。</p>
<ul>
<li>訓練性能
\begin{eqnarray*}
MSE_{train} = \frac{1}{n_{train}} \sum^{n_{train}}_{i=1} \left( \hat{y}_i^{(train)} - y^{(train)}_i \right)^2 \<br>
\end{eqnarray*}</li>
<li>テスト誤差
\begin{eqnarray*}
MSE_{test} = \frac{1}{n_{test}} \sum^{n_{test}}_{i=1} \left( \hat{y}_i^{(test)} - y^{(test)}_i \right)^2 \<br>
\end{eqnarray*}</li>
</ul>
<h5 id="未学習と過学習の見分け方">未学習と過学習の見分け方</h5>
<ul>
<li>訓練誤差もテスト誤差もどちらも小さい$\rightarrow$汎化しているモデル</li>
<li>訓練誤差は小さいがテスト誤差が大きい$\rightarrow$過学習</li>
<li>訓練誤差もテスト誤差もどちらも小さくならない$\rightarrow$未学習</li>
</ul>
<h5 id="ホールドアウト法">ホールドアウト法</h5>
<ul>
<li>有限のデータと学習用とテスト用の2つに分割し、「予測制度」や「誤り率」を推定するために使用する。
<ul>
<li>学習用を多くすればテスト用が減り学習精度は良くなるが、性能評価の精度は悪くない</li>
<li>逆にテスト用を多くすれば学習用が減少するので、学習そのものの精度が悪くなることになる</li>
<li>手元にデータが大量にある場合を除いて、良い性能評価を与えないという欠点がある</li>
</ul>
</li>
</ul>
<h4 id="グリッドサーチ">グリッドサーチ</h4>
<ul>
<li>全てのチューニングパラメータの組み合わせで評価値を算出</li>
<li>最も良い評価値を持つチューイングパラメータを持つ組み合わせを「いいモデルのパラメータ」として採用する
<code>$$ \begin{eqnarray*} \boldsymbol{m} &amp;=&amp; (m_1, m_2, \cdots, m_c)^T \\ \boldsymbol{\lambda} &amp;=&amp; (\lambda_1, \lambda_2, \cdots, \lambda_c)^T \end{eqnarray*} $$</code></li>
</ul>
<p>&lt;実装演習結果&gt;</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1">#seaborn設定</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="c1">#背景変更</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&#34;darkgrid&#34;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;grid.linestyle&#39;</span><span class="p">:</span> <span class="s1">&#39;--&#39;</span><span class="p">})</span>
<span class="c1">#大きさ(スケール変更)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&#34;paper&#34;</span><span class="p">)</span>

<span class="n">n</span><span class="o">=</span><span class="mi">100</span>

<span class="k">def</span> <span class="nf">true_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="mi">48</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">218</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="mi">315</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="mi">145</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">4</span>
    <span class="k">return</span> <span class="n">z</span> 

<span class="k">def</span> <span class="nf">linear_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="n">z</span> <span class="c1"># 真の関数からノイズを伴うデータを生成</span>

<span class="c1"># 真の関数からデータ生成</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">true_func</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># 　ノイズを加える</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> 
<span class="n">target</span> <span class="o">=</span> <span class="n">target</span>  <span class="o">+</span> <span class="n">noise</span>
</code></pre></div><p>KernelRidgeを使用して非線形関数でフィッティングを行う。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="n">p_kridge</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">p_kridge</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;kernel ridge&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1">#plt.plot(data, p, color=&#39;orange&#39;, marker=&#39;o&#39;, linestyle=&#39;-&#39;, linewidth=1, markersize=6)</span>
</code></pre></div><figure class="center"><img src="/image/hisenkei.png" width="500" height="300"/><figcaption>
            <h4>非線形関数でのフィッティング結果</h4>
        </figcaption>
</figure>

<h1 id="ロジスティック回帰モデル">ロジスティック回帰モデル</h1>
<p>&lt;講義内容サマリ&gt;</p>
<p>ロジスティック線形回帰モデルは、分類問題を解くための教師あり機械学習モデルである。
入力と$m$次元パラメータの線型結合をシグモイド関数に入力する。シグモイド関数の出力値は0〜1の間をとり、その出力値は（クラス1）に分類される確率を表現している。</p>
<h4 id="シグモイド関数">シグモイド関数</h4>
<p>\begin{eqnarray*}
\sigma (x) = \frac{1}{1 + \exp{(-ax)}}
\end{eqnarray*}</p>
<h4 id="ロジスティック線形回帰モデル">ロジスティック線形回帰モデル</h4>
<p>データYは確率が0.5以上ならば1, 0.5未満ならば0と予測することで、分類問題を解決する。
\begin{eqnarray*}
P(Y=1 | \boldsymbol{x}) = \sigma(w_0 + w_1 x_1 + \cdots + w_m x_m) \
\end{eqnarray*}</p>
<h4 id="最尤推定">最尤推定</h4>
<h5 id="ベルヌーイ分布について">ベルヌーイ分布について</h5>
<p>ベルヌーイ分布とは、数学において確率pで1, 確率(1-p)で0をとる離散確率分布である。
\begin{eqnarray*}
P(y) = p^{y} (1-p)^{1-y} \<br>
\end{eqnarray*}</p>
<h5 id="尤度関数について">尤度関数について</h5>
<p>1回の試行において、$y=y_1$になる確率
\begin{eqnarray*}
P(y) = p^{y} (1-p)^{1-y} \
\end{eqnarray*}
n回の試行において$y_1$〜$y_n$が得られた場合、その出力結果から確率pを求める際に、尤度関数を最大化するようなパラメータ（確率p）を求める。このことを最尤推定という。
\begin{eqnarray*}
P(y_1,y_2,\cdots, y_n;p) = \prod^{n}_{i=1} p^{y_i} (1-p)^{1-y_i} \
\end{eqnarray*}</p>
<h5 id="ロジスティック回帰モデルの最尤推定">ロジスティック回帰モデルの最尤推定</h5>
<p>確率pはシグモイド関数の出力結果となるため、推定するパラメータはシグモイド関数のインプットデータとなっている重みパラメータとなる。
尤度関数はパラメータのみに依存する関数のため、尤度関数を最大とするパラメータを探索する。
<code>$$ \begin{eqnarray*} P(y_1,y_2,\cdots, y_n|w_0,w_1, \cdots, w_m) &amp;=&amp; \prod^{n}_{i=1} p^{y_i} (1-p)^{1-y_i} \\  &amp; = &amp;  \prod^{n}_{i=1} \sigma (\boldsymbol{w}^T \boldsymbol{x}_i)^{y_i} (1- \sigma (\boldsymbol{w}^T \boldsymbol{x}_i))^{1-y_i} \\ &amp; = &amp; L (\boldsymbol{w}) \\ \end{eqnarray*} $$</code></p>
<ul>
<li>尤度関数を最愛とするパラメータを探すために、尤度関数の対数をとり-1倍する。
** 対数を取ることで積和の変換がなされ、計算が簡単になる
** 桁落ちを防止する
** -1倍をすることで、最大化から最小化となり、最小二乗法と平仄を合わせている</li>
</ul>
<p><code>$$ \begin{eqnarray*} E (w_0,w_1, \cdots, w_m) &amp;=&amp; - \log L(w_0,w_1, \cdots, w_m) \\ &amp;=&amp; - \sum^{n}_{i=1} {y_i \log p_i + (1 - y_i) \log (1 - p_i)}\\  \end{eqnarray*} $$</code></p>
<h5 id="勾配降下法">勾配降下法</h5>
<p>ロジスティック回帰モデル（最尤法）では、対数尤度関数をパラメータで微分して0となる値を求める必要があるが、解析的にその値を求めることは困難である。そのため、パラメータに初期値を与え、パラメータを更新していき、勾配が0となった部分で最適な解が求められたことになる。
\begin{equation*}
\boldsymbol{w}(k + 1) =  \boldsymbol{w}^k - \eta \frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol {w}} \<br>
\end{equation*}
※勾配降下法では、パラメータを更新するのにN個全てのデータに対する和を計算する必要があるため、データ量が膨大となった際に処理時間の増加等の懸念がある。データ量が膨大な場合は、確率的勾配降下法を利用する。</p>
<h5 id="確率的勾配降下法sgd">確率的勾配降下法（SGD）</h5>
<p>データをランダムに選びパラメータを更新していく。勾配降下法にてパラメータを1回更新するのと同じ計算量でパラメータをn回更新できるため、効率よく最適な解を探索できる。
\begin{eqnarray*}
\boldsymbol{w} (k + 1 ) = \boldsymbol{w}^k + \eta (y_i - p_i) \boldsymbol{x}_i \<br>
\end{eqnarray*}</p>
<p>&lt;実装演習結果&gt;</p>
<p>ロジスティック回帰の実習演習として、Kaggleが提供する「タイタニック号の生存者」に関するデータセットを用いて、ロジスティック回帰を実装した。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#タイタニックデータセットのインポート</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pandas</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;/content/drive/MyDrive/Colab Notebooks/機械学習/非線形回帰/train.csv&#34;</span><span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;/content/drive/MyDrive/Colab Notebooks/機械学習/非線形回帰/test.csv&#34;</span><span class="p">)</span>
</code></pre></div><p>データ欠損状況を確認して、欠損しているデータについては平均値で補間を実施する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#欠損状況の確認</span>
<span class="k">def</span> <span class="nf">kesson_table</span><span class="p">(</span><span class="n">df</span><span class="p">):</span> 
        <span class="n">null_val</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="n">percent</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="n">kesson_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">null_val</span><span class="p">,</span> <span class="n">percent</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">kesson_table_ren_columns</span> <span class="o">=</span> <span class="n">kesson_table</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;欠損数&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="p">:</span> <span class="s1">&#39;%&#39;</span><span class="p">})</span>
        <span class="k">return</span> <span class="n">kesson_table_ren_columns</span>
 
<span class="n">kesson_table</span><span class="p">(</span><span class="n">train_df</span><span class="p">)</span>
<span class="n">kesson_table</span><span class="p">(</span><span class="n">test_df</span><span class="p">)</span>
</code></pre></div><p><figure class="center"><img src="/image/train_%e6%ac%a0%e6%90%8d.png" width="200" height="300"/><figcaption>
            <h4>trainデータの欠損状況</h4>
        </figcaption>
</figure>

<figure class="center"><img src="/image/test_%e6%ac%a0%e6%90%8d.png" width="200" height="300"/><figcaption>
            <h4>testデータの欠損状況</h4>
        </figcaption>
</figure>
</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># Ageは説明変数へ利用するため、欠損については中央値で補完する。</span>
<span class="c1">#年齢の欠損値を男女の平均年齢で補間</span>
<span class="n">age_train_mean</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Sex&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">Age</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">fillage</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">Sex</span> <span class="o">==</span> <span class="s1">&#39;male&#39;</span><span class="p">:</span>
          <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">age_train_mean</span><span class="p">[</span><span class="s1">&#39;male&#39;</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">Sex</span> <span class="o">==</span> <span class="s1">&#39;female&#39;</span><span class="p">:</span>
          <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">age_train_mean</span><span class="p">[</span><span class="s1">&#39;female&#39;</span><span class="p">])</span>
<span class="c1">#補間を実行する</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">Age</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="n">train_df</span><span class="o">.</span><span class="n">Age</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fillage</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="c1">#Ageについて補間されたことを確認</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div><figure class="center"><img src="/image/train_hokan.png" width="800" height="220"/><figcaption>
            <h4>trainデータのAgeの補間結果を確認</h4>
        </figcaption>
</figure>

<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#テスト用データのAgeについても同様に補間</span>
<span class="n">age_test_mean</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Sex&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">Age</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> 
<span class="n">test_df</span><span class="o">.</span><span class="n">Age</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">test_df</span><span class="p">[</span><span class="n">test_df</span><span class="o">.</span><span class="n">Age</span><span class="o">.</span><span class="n">isnull</span><span class="p">()]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fillage</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1">#テスト用データのFareには1件欠損存在するため、同様に平均値で補間</span>
<span class="n">Fare_test_mean</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;Fare&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">Fare</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> 
<span class="n">test_df</span><span class="o">.</span><span class="n">Fare</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">test_df</span><span class="o">.</span><span class="n">Fare</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1">#Fareについて補間されたことを確認</span>
<span class="n">test_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div><figure class="center"><img src="/image/test_hokan.png" width="800" height="220"/><figcaption>
            <h4>testデータのAgeの補間結果を確認</h4>
        </figcaption>
</figure>

<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#Femaleカラムを追加し、Sex要素のmale/femaleを1/0に変換して、要素として追加する</span>
<span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;Sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span> <span class="p">{</span><span class="s1">&#39;male&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span> <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;Female&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s1">&#39;Sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span> <span class="p">{</span><span class="s1">&#39;male&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;female&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span> <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="c1">#不要なカラムを落として、ロジスティック回帰にてFittingを実施</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">,</span><span class="s1">&#39;Name&#39;</span><span class="p">,</span><span class="s1">&#39;Sex&#39;</span><span class="p">,</span><span class="s1">&#39;Ticket&#39;</span><span class="p">,</span><span class="s1">&#39;Cabin&#39;</span><span class="p">,</span><span class="s1">&#39;Survived&#39;</span><span class="p">,</span><span class="s1">&#39;Embarked&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">Survived</span>

<span class="bp">cls</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="bp">cls</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div><p>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
intercept_scaling=1, l1_ratio=None, max_iter=100,
multi_class=&lsquo;auto&rsquo;, n_jobs=None, penalty=&lsquo;l2&rsquo;,
random_state=None, solver=&lsquo;lbfgs&rsquo;, tol=0.0001, verbose=0,
warm_start=False)
学習したモデルの精度を表示する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="bp">cls</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div><p>結果：0.7968574635241302</p>
<p>最後にテストデータから生存者を予測する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1">#テストデータから生存者を予測</span>
<span class="n">test_df_d</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">,</span><span class="s1">&#39;Name&#39;</span><span class="p">,</span><span class="s1">&#39;Sex&#39;</span><span class="p">,</span><span class="s1">&#39;Ticket&#39;</span><span class="p">,</span><span class="s1">&#39;Cabin&#39;</span><span class="p">,</span><span class="s1">&#39;Embarked&#39;</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">test_predict</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_df_d</span><span class="p">)</span>

<span class="c1">#テスト結果とマージ</span>
<span class="n">test_df_a</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&#34;/content/drive/MyDrive/Colab Notebooks/機械学習/非線形回帰/test.csv&#34;</span><span class="p">)</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">:</span> <span class="n">test_df_a</span><span class="p">[</span><span class="s1">&#39;PassengerId&#39;</span><span class="p">],</span> <span class="s1">&#39;Survived&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_predict</span><span class="p">)})</span>
<span class="n">result_df</span>
</code></pre></div><figure class="center"><img src="/image/test_fit.png" width="200" height="300"/><figcaption>
            <h4>testデータに対する生存者予想の結果</h4>
        </figcaption>
</figure>

<h1 id="主成分分析">主成分分析</h1>
<p>&lt;講義内容サマリ&gt;</p>
<p>多変量データの持つ構造をより少数個の指標に圧縮し、小さな次元の情報に要約することで、全体の把握が容易になる。そのためには各変数のもつ共通の要素を抜き出して要約をすれば良い。
このように、変数を加工し、そこから要約された情報を取り出す手法が主成分分析です。</p>
<p>主成分分析では、多変量データを統合し、新たな総合指標を作り出すための手法ですが、多くの変数に重み（ウェイト）をつけて少数の合成変数（主成分）を作成する。情報の量を分散の大きさと捉え、線形変換後の変数の分散が最大となるような射影軸を探索する。</p>
<p><code>$$ \begin{eqnarray*} \boldsymbol{s}_j &amp;=&amp; (s_{1j}, s_{2j}, \cdots, s_{nj})^T = \bar{X} \boldsymbol{a}_j  \\  \boldsymbol{a}_j &amp;\in&amp; \mathbb{R}^m \\ \end{eqnarray*} $$</code></p>
<p>探索には、制約（ノルムが1）付き最適化問題を解くが、ラグランジュ関数を最大にする係数ベクトルを探索する（微分して0となる点）。
ラグランジュ関数を微分して最適解を求めていくが、元データの分散共分散行列の固有値と固有ベクトルが、制約付き最適化問題の解となる。</p>
<p>&lt;実装演習結果&gt;</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">cancer_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;/content/drive/MyDrive/Colab Notebooks/機械学習/data/cancer.csv&#39;</span><span class="p">)</span>
<span class="n">cancer_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Unnamed: 32&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div><p>説明変数を3列目以降、目的変数を2列目（診断結果：良性B/悪性M）とする。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># 目的変数の抽出</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">cancer_df</span><span class="o">.</span><span class="n">diagnosis</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">d</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="s1">&#39;M&#39;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># 説明変数の抽出</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">cancer_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;radius_mean&#39;</span><span class="p">:]</span>

<span class="c1"># 学習用とテスト用でデータを分離</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 標準化</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># ロジスティック回帰で学習</span>
<span class="n">logistic</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">logistic</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 検証</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Train score: {:.3f}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logistic</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Test score: {:.3f}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logistic</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Confustion matrix:</span><span class="se">\n</span><span class="s1">{}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">logistic</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">))))</span>
</code></pre></div><p>Train score: 0.988
Test score: 0.972
Confustion matrix:
[[89  1]
[ 3 50]]</p>
<p>検証スコア97%で分類できることを確認した。
主成分分析としてPCAオブジェクトを生成し、次元数を2まで削減する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="c1"># PCA</span>
<span class="c1"># 次元数2まで圧縮</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_train_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;X_train_pca shape: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train_pca</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="c1"># X_train_pca shape: (426, 2)</span>

<span class="c1"># 寄与率</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;explained variance ratio: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>
<span class="c1"># explained variance ratio: [ 0.43315126  0.19586506]</span>

<span class="c1"># 散布図にプロット</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_train_pca</span><span class="p">)</span>
<span class="n">temp</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="n">temp</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">temp</span><span class="p">[</span><span class="n">temp</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span> <span class="c1"># 良性は○でマーク</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;^&#39;</span><span class="p">)</span> <span class="c1"># 悪性は△でマーク</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;PC 1&#39;</span><span class="p">)</span> <span class="c1"># 第1主成分をx軸</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;PC 2&#39;</span><span class="p">)</span> <span class="c1"># 第2主成分をy軸</span>
</code></pre></div><figure class="center"><img src="/image/pca_rlt.png" width="600" height="400"/><figcaption>
            <h4>主成分分析による可視化の結果</h4>
        </figcaption>
</figure>

<p>PCAは教師なし学習であり、適切な主成分を見つけるタイミングで、教師データであるクラス情報を使わない。特徴量間の相関を見ているのみである。</p>
<h1 id="アルゴリズム">アルゴリズム</h1>
<p>&lt;サマリ&gt;</p>
<h4 id="k近傍法">k近傍法</h4>
<p>k近傍法とは、クラスタ分析手法の1つであり、与えられた学習データをプロットしておき、未知のデータに対して距離が近い順に任意のk個を取得し、その多数決でデータが属するクラスを推定するもの。k近傍法には、近傍点の数・データポイント間の距離、という2つの重要なパラメータがある。説明変数が多い場合は、学習速度が遅くなる傾向があるため、次元削減等のデータ前処理が必要な場合がある。</p>
<p>&lt;実装演習結果&gt;</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
 
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mglearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">make_forge</span><span class="p">()</span>
 
<span class="c1"># train_test_splitで分割する</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
 
<span class="c1"># インスタンスの生成、3つの近傍点で投票を行う</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
 
<span class="c1"># 学習する</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div><p>どのような分類がされたかについて、確認をする。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
 
<span class="k">for</span> <span class="n">n_neighbors</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">axes</span><span class="p">):</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">n_neighbors</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># plot_2d_separator: 色で二分割</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=.</span><span class="mi">4</span><span class="p">)</span>
    <span class="c1"># discrete_scatter: 要素を分布</span>
    <span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&#34;{} neighbor(s)&#34;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_neighbors</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;feature 0&#34;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;feature 1&#34;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div><figure class="center"><img src="/image/k-kinbou.png" width="1000" height="200"/><figcaption>
            <h4>k-means法によるクラスタ分析の結果</h4>
        </figcaption>
</figure>

<h4 id="k-means法">k-means法</h4>
<p>k-means法は教師なし学習のクラスタ分析手法の1つで、最も広く用いられているクラスタリング手法の1つである。k-meansは任意にポイントした重心からの距離によってクラスタリングを実施する。</p>
<ul>
<li>アルゴリズム
<ul>
<li>各クラスタ中心の初期値を設定する</li>
<li>各データ点に対して、各クラスタ中心との距離を計算し、最も距離が近いクラスタを割り当てる。</li>
<li>各クラスタの平均ベクトル（中心）を計算する</li>
<li>収束するまで上記の処理を繰り返し実施する</li>
</ul>
</li>
</ul>
<p>&lt;実装演習結果&gt;
scikit learnにて、クラスタリング用のデータを作成する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="ch">#!pip install mglearn</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">mglearn</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
</code></pre></div><p>クラスタセンタを2と4に指定して、クラスタに分割した結果を表示する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1">#クラスタセンサを2つに指定</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">assignments</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">assignments</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1">#クラスタセンサを4つに指定</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">assignments</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">assignments</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div><figure class="center"><img src="/image/k-means.png" width="1000" height="300"/><figcaption>
            <h4>k-means法によるクラスタ分析の結果</h4>
        </figcaption>
</figure>

<h1 id="サポートベクタマシン">サポートベクタマシン</h1>
<p>&lt;サマリ&gt;</p>
<p>サポートベクタマシンでは、正しい分類基準を見つけるために、「マージン最大化」という考え方を用いる。マージンとは、「判別する境界とデータの距離」のことを言う。なお、境界線と最も近くにあるデータを「サポートベクタ」と呼ぶ。このサポートベクタとのマージンを最大化するように境界を引き、クラスタ分類を行う。サポートベクタ以外のデータの値が多少変化したとしても、分類のための境界線の位置は一切変わらない。SVMは様々なkでーたセットに対してうまく機能するモデルであり、データにわずかな特徴量しかない場合でも、複雑な境界を生成することが可能。</p>
<h4 id="カーネルトリックkernel-trick">カーネルトリック(kernel trick)</h4>
<p>カーネルトリックとは、特徴空間中のデータ座標の明示的なけいさんを経由せずに、特徴空間い置ける関係性をデータから直接計算する手段を与える。つまり、かネールトリックを使うことで、明示的な座標の計算を経るよりもしばしば計算量が少なくて住むようになる。</p>
<ul>
<li>
<p>サポートベクタマシーン（SVM）のscikit-learnでの扱い</p>
<ul>
<li>SVC：SVM Classification(分類問題に適用する)</li>
<li>SVR：SVM Regression(回帰問題に適用する)</li>
</ul>
</li>
<li>
<p>サポートベクタマシーンのデメリット</p>
<ul>
<li>データの前処理とパラメータチューニングを注意深くする必要がある</li>
<li>予測結果の検証が困難（予測結果に対する説明が難しい）</li>
</ul>
</li>
</ul>
<p>&lt;実装演習結果&gt;
scikit learnにて、クラスタリング用のデータを作成する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="n">mglearn</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">mglearn</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">centers</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">%</span> <span class="mi">2</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Feature 0&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Feature 1&#34;</span><span class="p">)</span>
</code></pre></div><figure class="center"><img src="/image/svm1.png" width="450" height="300"/><figcaption>
            <h4>分類用テストデータ</h4>
        </figcaption>
</figure>

<p>まずは線形にてクラスタ分類を試みる。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="n">linear_svn</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_2d_separator</span><span class="p">(</span><span class="n">linear_svn</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&#34;Feature 0&#34;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&#34;Feature 1&#34;</span><span class="p">)</span>
</code></pre></div><p><figure class="center"><img src="/image/svm2.png" width="450" height="300"/><figcaption>
            <h4>分類用テストデータを線形にて分類した結果</h4>
        </figcaption>
</figure>

全くクラスタ分類できていないことがわかる。
このままでは分離ができないため、入力特徴量を拡張子、3次元のデータへと拡張子、サポートベクタマシーンにて評価する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span><span class="p">,</span> <span class="n">axes3d</span>

<span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">figure</span><span class="p">,</span> <span class="n">elev</span> <span class="o">=</span> <span class="o">-</span><span class="mi">152</span><span class="p">,</span> <span class="n">azim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">26</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">y</span> <span class="o">==</span> <span class="mi">0</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;feature0&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;feature1&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&#34;feature1 ** 2&#34;</span><span class="p">)</span>
</code></pre></div><figure class="center"><img src="/image/svm3.png" width="450" height="300"/><figcaption>
            <h4>分類用テストデータを3次元に拡張</h4>
        </figcaption>
</figure>

<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">linear_svm_3d</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_new</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span> <span class="o">=</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">intercept_</span>

<span class="n">figure</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">figure</span><span class="p">,</span> <span class="n">elev</span> <span class="o">=</span> <span class="o">-</span><span class="mi">152</span><span class="p">,</span> <span class="n">azim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">26</span><span class="p">)</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span> <span class="mi">2</span> <span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span><span class="p">,</span> <span class="n">X_new</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span> <span class="mi">2</span> <span class="p">,</span> <span class="mi">50</span><span class="p">)</span>

<span class="n">XX</span><span class="p">,</span> <span class="n">YY</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">)</span>
<span class="n">ZZ</span> <span class="o">=</span> <span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">XX</span> <span class="o">+</span> <span class="n">coef</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">YY</span> <span class="o">+</span> <span class="n">intercept</span><span class="p">)</span> <span class="o">/</span> <span class="o">-</span><span class="n">coef</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">,</span> <span class="n">rstride</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">cstride</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span><span class="n">X_new</span><span class="p">[</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="mi">2</span><span class="p">],)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;feature0&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;feature1&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s2">&#34;feature1 ** 2&#34;</span><span class="p">)</span>
</code></pre></div><p><figure class="center"><img src="/image/svm4.png" width="450" height="300"/><figcaption>
            <h4>3次元に拡張したデータにSVMを適用した結果</h4>
        </figcaption>
</figure>

2次元では線形分離できなかったデータを3次元に拡張することで、SVMを使用して線形モデル（平面）にて分離することが可能となった。最後に、線形モデルを3次元に適用した結果を2次元に落として決定境界を描写する。</p>
<div class="highlight"><pre class="chroma"><code class="language-python" data-lang="python"><span class="n">ZZ</span> <span class="o">=</span> <span class="n">YY</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">dec</span> <span class="o">=</span> <span class="n">linear_svm_3d</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">XX</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">YY</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">ZZ</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">XX</span><span class="p">,</span> <span class="n">YY</span><span class="p">,</span> <span class="n">dec</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">XX</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="n">dec</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dec</span><span class="o">.</span><span class="n">max</span><span class="p">()],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mglearn</span><span class="o">.</span><span class="n">cm2</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:</span> <span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&#34;feature0&#34;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&#34;feature1&#34;</span><span class="p">)</span>
</code></pre></div><figure class="center"><img src="/image/svm5.png" width="750" height="300"/><figcaption>
            <h4>2次元上での決定境界</h4>
        </figcaption>
</figure>

</article>

        </main><footer id="footer">
    Copyright © 2021 Yuki Ono
</footer>
</body>
</html>
