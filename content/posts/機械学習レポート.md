---
title: "機械学習レポート"
date: 2021-06-12T01:18:19+09:00
draft: false
mathjax: true
---
[1.線形回帰モデル](#線形回帰モデル)

[2.非線形回帰モデル](#非線形回帰モデル)

[3.ロジスティック回帰モデル](#ロジスティック回帰モデル)

[4.主成分分析](#主成分分析)

[5.アルゴリズム](#アルゴリズム)

[6.サポートベクタマシン](#サポートベクタマシン)


# 線形回帰モデル

<講義内容サマリ>
### 回帰問題
回帰モデルではある入力値（離散あるいは連続）から出力値（連続）を予測する問題であり、線形関数（直線）で予測するものを線形回帰（単回帰）という。
* 回帰で扱うデータ
  * 入力値（各要素を説明変数または特徴量と呼ぶ）
     * m次元ベクトル（m=1の場合はスカラー） 
  * 出力値（目的変数）
     * スカラー値（目的変数）

**入力値（説明変数）**
`$$
\begin{eqnarray*}
\boldsymbol{x} &=& (x_1, x_2, \cdots, x_m)^{T} \\
\end{eqnarray*}
$$`

**出力値（目的変数）**
`$$
\begin{eqnarray*}
y \in \mathbb{R}^m \\
\end{eqnarray*}
$$`

### 線形回帰モデルとは
線形回帰モデルとは、機械学習モデルの一つであり、教師あり学習の回帰手法である。
入力値に対して$m$次元パラメータの線型結合を出力するモデルである。
パラメータとは、モデルに含まれる推定すべき未知のパラメータであり、特徴量が予測値に対してどのように影響を与えるかを決定する重み値の集合である。
線形回帰モデルでは、この未知のパラメータに対して、最小二乗法により推定する。

**パラメータ**
`$$
\begin{eqnarray*}
\boldsymbol{w} &=& (w_1, w_2, \cdots, w_m)^{T} \in \mathbb{R}^m \\
\end{eqnarray*}
$$`

**線型結合**
`$$
\begin{eqnarray*}
 \hat{y} = \boldsymbol{w}^T \boldsymbol{x} + w_0 = \sum^{m}_{j=1} w_j x_j + w_0 \\
\end{eqnarray*}
$$`

### データの分割とモデルの汎化性能測定
教師用データについて、学習用データと検証用データにそれぞれ分割する。モデルの汎化性能（Generalization）を測定するためで、データへの当てはまりの良さではなく、未知のインプットデータに対しての精度の高さを測定する。
* 学習用データ：機械学習モデルの学習に利用するデータ
* 検証用データ：学習済みモデルの制度を検証するためのデータ

### 線形回帰モデルパラメータの推定
モデルパラメータの推定は、以下の平均二乗誤差MSE$_{train}$（残渣平方和：データとモデル出力の二乗誤差の和）を最小化するパラメータを探索する。
`$$
\begin{eqnarray*}
 \rm{MSE}_{train} = \frac{1}{n_{train}} \sum^{n_{train}}_{i=1} (\hat{y}^{(train)}_i - y^{(train)}_i)^2 \\ 
\end{eqnarray*}
$$`

MSEを最小とするような$w$を求めるため、以下の方程式を解く。
\begin{eqnarray*}
 \frac{\partial}{\partial w}\rm{MST_{train}} = 0 \\
\end{eqnarray*}
左辺について行列変形を実施する。
`
\begin{eqnarray*}
 \frac{\partial}{\partial w}\rm{MST_{train}} &=&  \frac{\partial}{\partial w} \left(\frac{1}{n_{\rm{train}}} \sum^{n_{\rm{(train)}}}_{i=1} \left(\hat{y}^{\rm{(train)}}_i - y^{\rm{(train)}}_i\right)^2 \right) \\
&=& \frac{\partial}{\partial \boldsymbol{w}} \left( \left(X^{n_{\rm{(train)}}} \boldsymbol{w} - \boldsymbol{y}^{(\rm{train})} \right)^T \left(X^{n_{\rm{(train)}}} \boldsymbol{w} - \boldsymbol{y}^{(\rm{(train)})} \right) \right) \\
&=& 0  \\
\end{eqnarray*}
`
これを解くと以下の回帰係数の式が得られる。
* 回帰係数
\begin{eqnarray*}
 \boldsymbol{\hat{w}} &=& \left({X^{\rm{(train)}}}^{T} X^{\rm{(train)}}\right)^{-1} {X^{\rm{(train)}}}^{T} \boldsymbol{y}^{\rm{(train)}} \\
\end{eqnarray*}

* 予測値
\begin{eqnarray*}
 \boldsymbol{\hat{y}} &=& X\left({X^{\rm{(train)}}}^{T} X^{\rm{(train)}}\right)^{-1} {X^{\rm{(train)}}}^{T} \boldsymbol{y}^{\rm{(train)}} \\
\end{eqnarray*}




# 非線形回帰モデル

<講義内容サマリ>

線形回帰で実現できないような複雑な非線形構造を内在する現象に対して、非線形回帰モデリングを実施する。
非線形回帰モデルでは、回帰関数として基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線型結合を使用する。未知パラメータは線形回帰モデルと応用に最小二乗法や最尤法によって推定する。
基底展開法も線形回帰と同じ枠組みで推定可能である。

\begin{eqnarray*}
 y_i = w_0 + \sum^{m}_{j=1} w_j \phi_j (\boldsymbol{x}_i) + \epsilon_i \\
\end{eqnarray*}

* よく使われる基底関数
  * 多項式関数
  * ガウス型基底関数
  * スプライン関数/Bスプライン関数

説明変数
\begin{eqnarray*}
 x_i = (x_{i1},x_{i2},\cdots, x_{im}) \in \mathbb{R}^m \\
\end{eqnarray*}
非線形関数ベクトル
\begin{eqnarray*}
 \phi (\boldsymbol{x_i}) = (\phi_1(\boldsymbol{x}_i), \phi_2 (\boldsymbol{x}_i), \cdots, \phi_k (\boldsymbol{x}_i))^T \in \mathbb{R}^k \\
\end{eqnarray*}
非線形関数の計画行列
\begin{eqnarray*}
 \Phi^{(train)} = (\Phi (\boldsymbol{x}_1), \Phi (\boldsymbol{x}_2), \cdots, \Phi (\boldsymbol{x}_n))^T \in \mathbb{R}^{n \times k} \\
\end{eqnarray*}
最尤法による予測値
\begin{eqnarray*}
 \hat{\boldsymbol{y}} = \Phi \left( \Phi^{(train)T} \Phi^{(train)} \right)^{-1} \Phi^{(train)T} \boldsymbol{y}^{(train)} \\
\end{eqnarray*}

#### 未学習（underfitting) と過学習 (overfitting)

* 学習データに対して、十分小さな誤差が得られないモデル$\rightarrow$未学習
  * （対策）モデルの表現力が低いため、表現力の高いモデルを利用する
* 小さな誤差は得られたけど、テスト集合誤差との差が大きいモデル$\rightarrow$過学習
  * (対策1) 学習データの数を増やす
  * (対策2) 不要な基底関数（変数）を削除して表現力を抑止
  * (対策3) 正則化法を利用して表現力を抑止

##### （対策2）不要な基底関数を削除する
* 基底関数の数、位置やバンド幅によるモデルの複雑さが変化する
* 解きたい問題に対して多くの基底関数を用意してしまうと過学習の問題が起こるため、適切な基底関数を用意する

##### （対策3）正則化法（罰則化法）
* 「モデルの複雑さに伴って、その値が大きくなる正則化項（罰則項）を課した関数」を最小化する
\begin{eqnarray*}
 S_{\gamma} = \left(\boldsymbol{y} - \Phi \boldsymbol{w}\right)^T \left( \boldsymbol{y} - \Phi \boldsymbol{w} \right) + \gamma R(\boldsymbol{w}) 
\end{eqnarray*}
基底関数の数kが増加するとパラメータが増加し、残渣が減少する（モデルが複雑化）。

* 正則化
  * L2ノルムを利用 $\rightarrow$ Ridge推定量
  * L1ノルムを利用 $\rightarrow$ Lasso推定量

#### 汎化性能
適切なモデル（汎化性能が高いモデル）は交差検証法にて決定する。汎化性能とは、学習に使用した入力だけではなく、これまで見たことのない新たな入力に対する予測性能のことである。

* 訓練性能
\begin{eqnarray*}
 MSE_{train} = \frac{1}{n_{train}} \sum^{n_{train}}_{i=1} \left( \hat{y}_i^{(train)} - y^{(train)}_i \right)^2 \\
\end{eqnarray*}
* テスト誤差
\begin{eqnarray*}
 MSE_{test} = \frac{1}{n_{test}} \sum^{n_{test}}_{i=1} \left( \hat{y}_i^{(test)} - y^{(test)}_i \right)^2 \\
\end{eqnarray*}

##### 未学習と過学習の見分け方
* 訓練誤差もテスト誤差もどちらも小さい$\rightarrow$汎化しているモデル
* 訓練誤差は小さいがテスト誤差が大きい$\rightarrow$過学習
* 訓練誤差もテスト誤差もどちらも小さくならない$\rightarrow$未学習

##### ホールドアウト法
* 有限のデータと学習用とテスト用の2つに分割し、「予測制度」や「誤り率」を推定するために使用する。
  * 学習用を多くすればテスト用が減り学習精度は良くなるが、性能評価の精度は悪くない
  * 逆にテスト用を多くすれば学習用が減少するので、学習そのものの精度が悪くなることになる
  * 手元にデータが大量にある場合を除いて、良い性能評価を与えないという欠点がある

#### グリッドサーチ
* 全てのチューニングパラメータの組み合わせで評価値を算出
* 最も良い評価値を持つチューイングパラメータを持つ組み合わせを「いいモデルのパラメータ」として採用する
`$$
\begin{eqnarray*}
 \boldsymbol{m} &=& (m_1, m_2, \cdots, m_c)^T \\
 \boldsymbol{\lambda} &=& (\lambda_1, \lambda_2, \cdots, \lambda_c)^T
\end{eqnarray*}
$$`

# ロジスティック回帰モデル

<講義内容サマリ>

ロジスティック線形回帰モデルは、分類問題を解くための教師あり機械学習モデルである。
入力と$m$次元パラメータの線型結合をシグモイド関数に入力する。シグモイド関数の出力値は0〜1の間をとり、その出力値は（クラス1）に分類される確率を表現している。
#### シグモイド関数
\begin{eqnarray*}
 \sigma (x) = \frac{1}{1 + \exp{(-ax)}}
\end{eqnarray*}

#### ロジスティック線形回帰モデル
データYは確率が0.5以上ならば1, 0.5未満ならば0と予測することで、分類問題を解決する。
\begin{eqnarray*}
 P(Y=1 | \boldsymbol{x}) = \sigma(w_0 + w_1 x_1 + \cdots + w_m x_m) \\ 
\end{eqnarray*}

#### 最尤推定
##### ベルヌーイ分布について
ベルヌーイ分布とは、数学において確率pで1, 確率(1-p)で0をとる離散確率分布である。
\begin{eqnarray*}
 P(y) = p^{y} (1-p)^{1-y} \\
\end{eqnarray*}

##### 尤度関数について
1回の試行において、$y=y_1$になる確率
\begin{eqnarray*}
  P(y) = p^{y} (1-p)^{1-y} \\ 
\end{eqnarray*}
n回の試行において$y_1$〜$y_n$が得られた場合、その出力結果から確率pを求める際に、尤度関数を最大化するようなパラメータ（確率p）を求める。このことを最尤推定という。
\begin{eqnarray*}
  P(y_1,y_2,\cdots, y_n;p) = \prod^{n}_{i=1} p^{y_i} (1-p)^{1-y_i} \\ 
\end{eqnarray*}

##### ロジスティック回帰モデルの最尤推定
確率pはシグモイド関数の出力結果となるため、推定するパラメータはシグモイド関数のインプットデータとなっている重みパラメータとなる。
尤度関数はパラメータのみに依存する関数のため、尤度関数を最大とするパラメータを探索する。
`$$
\begin{eqnarray*}
  P(y_1,y_2,\cdots, y_n|w_0,w_1, \cdots, w_m) &=& \prod^{n}_{i=1} p^{y_i} (1-p)^{1-y_i} \\ 
& = &  \prod^{n}_{i=1} \sigma (\boldsymbol{w}^T \boldsymbol{x}_i)^{y_i} (1- \sigma (\boldsymbol{w}^T \boldsymbol{x}_i))^{1-y_i} \\
& = & L (\boldsymbol{w}) \\
\end{eqnarray*}
$$`
* 尤度関数を最愛とするパラメータを探すために、尤度関数の対数をとり-1倍する。
  ** 対数を取ることで積和の変換がなされ、計算が簡単になる
  ** 桁落ちを防止する
  ** -1倍をすることで、最大化から最小化となり、最小二乗法と平仄を合わせている

`$$
\begin{eqnarray*}
 E (w_0,w_1, \cdots, w_m) &=& - \log L(w_0,w_1, \cdots, w_m) \\
&=& - \sum^{n}_{i=1} {y_i \log p_i + (1 - y_i) \log (1 - p_i)}\\ 
\end{eqnarray*}
$$`

##### 勾配降下法
ロジスティック回帰モデル（最尤法）では、対数尤度関数をパラメータで微分して0となる値を求める必要があるが、解析的にその値を求めることは困難である。そのため、パラメータに初期値を与え、パラメータを更新していき、勾配が0となった部分で最適な解が求められたことになる。
\begin{equation*}
 \boldsymbol{w}(k + 1) =  \boldsymbol{w}^k - \eta \frac{\partial E(\boldsymbol{w})}{\partial \boldsymbol {w}} \\
\end{equation*}
※勾配降下法では、パラメータを更新するのにN個全てのデータに対する和を計算する必要があるため、データ量が膨大となった際に処理時間の増加等の懸念がある。データ量が膨大な場合は、確率的勾配降下法を利用する。

##### 確率的勾配降下法（SGD）
データをランダムに選びパラメータを更新していく。勾配降下法にてパラメータを1回更新するのと同じ計算量でパラメータをn回更新できるため、効率よく最適な解を探索できる。
\begin{eqnarray*}
 \boldsymbol{w} (k + 1 ) = \boldsymbol{w}^k + \eta (y_i - p_i) \boldsymbol{x}_i \\
\end{eqnarray*}


# 主成分分析

<講義内容サマリ>

多変量データの持つ構造をより少数個の指標に圧縮し、小さな次元の情報に要約することで、全体の把握が容易になる。そのためには各変数のもつ共通の要素を抜き出して要約をすれば良い。
このように、変数を加工し、そこから要約された情報を取り出す手法が主成分分析です。

主成分分析では、多変量データを統合し、新たな総合指標を作り出すための手法ですが、多くの変数に重み（ウェイト）をつけて少数の合成変数（主成分）を作成する。情報の量を分散の大きさと捉え、線形変換後の変数の分散が最大となるような射影軸を探索する。

`$$
\begin{eqnarray*}
 \boldsymbol{s}_j &=& (s_{1j}, s_{2j}, \cdots, s_{nj})^T = \bar{X} \boldsymbol{a}_j  \\ 
\boldsymbol{a}_j &\in& \mathbb{R}^m \\
\end{eqnarray*}
$$`

探索には、制約（ノルムが1）付き最適化問題を解くが、ラグランジュ関数を最大にする係数ベクトルを探索する（微分して0となる点）。
ラグランジュ関数を微分して最適解を求めていくが、元データの分散共分散行列の固有値と固有ベクトルが、制約付き最適化問題の解となる。



# アルゴリズム

<サマリ>

#### k近傍法
k近傍法とは、クラスタ分析手法の1つであり、与えられた学習データをプロットしておき、未知のデータに対して距離が近い順に任意のk個を取得し、その多数決でデータが属するクラスを推定するもの。k近傍法には、近傍点の数・データポイント間の距離、という2つの重要なパラメータがある。説明変数が多い場合は、学習速度が遅くなる傾向があるため、次元削減等のデータ前処理が必要な場合がある。

#### k-means法
k-means法は教師なし学習のクラスタ分析手法の1つで、最も広く用いられているクラスタリング手法の1つである。k-meansは任意にポイントした重心からの距離によってクラスタリングを実施する。
* アルゴリズム 
  * 各クラスタ中心の初期値を設定する
  * 各データ点に対して、各クラスタ中心との距離を計算し、最も距離が近いクラスタを割り当てる。
  *	各クラスタの平均ベクトル（中心）を計算する
  * 収束するまで上記の処理を繰り返し実施する


# サポートベクタマシン

<サマリ>

サポートベクタマシンでは、正しい分類基準を見つけるために、「マージン最大化」という考え方を用いる。マージンとは、「判別する境界とデータの距離」のことを言う。なお、境界線と最も近くにあるデータを「サポートベクタ」と呼ぶ。このサポートベクタとのマージンを最大化するように境界を引き、クラスタ分類を行う。サポートベクタ以外のデータの値が多少変化したとしても、分類のための境界線の位置は一切変わらない。SVMは様々なkでーたセットに対してうまく機能するモデルであり、データにわずかな特徴量しかない場合でも、複雑な境界を生成することが可能。

#### カーネルトリック(kernel trick)
カーネルトリックとは、特徴空間中のデータ座標の明示的なけいさんを経由せずに、特徴空間い置ける関係性をデータから直接計算する手段を与える。つまり、かネールトリックを使うことで、明示的な座標の計算を経るよりもしばしば計算量が少なくて住むようになる。

* サポートベクタマシーン（SVM）のscikit-learnでの扱い
  * SVC：SVM Classification(分類問題に適用する)
  * SVR：SVM Regression(回帰問題に適用する)

* サポートベクタマシーンのデメリット
  * データの前処理とパラメータチューニングを注意深くする必要がある
  * 予測結果の検証が困難（予測結果に対する説明が難しい）





